{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c076a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Le dossier data/csv contient dÃ©jÃ  8 fichier(s).\n",
      "Le tÃ©lÃ©chargement sera ignorÃ©.\n",
      "Chemin d'accÃ¨s aux fichiers du dataset: /home/bapt/code/Monitor-the-Reactor/Explo/data/csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# --- DÃ©finition des chemins ---\n",
    "# Le dossier principal qui contiendra les donnÃ©es\n",
    "DATA_DIR = \"data\"\n",
    "# Le dossier de destination spÃ©cifique au tÃ©lÃ©chargement Kaggle\n",
    "CSV_DIR = os.path.join(DATA_DIR, \"csv\")\n",
    "# Le handle Kaggle pour le dataset\n",
    "KAGGLE_HANDLE = \"afrniomelo/tep-csv\"\n",
    "\n",
    "# --- 1. VÃ©rification et CrÃ©ation des Dossiers ---\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    # !mkdir data\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print(f\"Dossier crÃ©Ã©: {DATA_DIR}\")\n",
    "\n",
    "if not os.path.exists(CSV_DIR):\n",
    "    # !mkdir data/csv\n",
    "    os.makedirs(CSV_DIR, exist_ok=True)\n",
    "    print(f\"Dossier crÃ©Ã©: {CSV_DIR}\")\n",
    "\n",
    "\n",
    "# --- 2. VÃ©rification du Contenu avant le TÃ©lÃ©chargement ---\n",
    "# On vÃ©rifie si le dossier de destination contient dÃ©jÃ  des fichiers (autres que les fichiers cachÃ©s/systÃ¨me)\n",
    "existing_files = [f for f in os.listdir(CSV_DIR) if not f.startswith('.')]\n",
    "\n",
    "if existing_files:\n",
    "    print(f\"âœ… Le dossier {CSV_DIR} contient dÃ©jÃ  {len(existing_files)} fichier(s).\")\n",
    "    print(\"Le tÃ©lÃ©chargement sera ignorÃ©.\")\n",
    "\n",
    "    # Optional: Si vous devez obtenir le chemin d'accÃ¨s au dossier pour la suite du script,\n",
    "    # vous pouvez le dÃ©finir ici.\n",
    "    path = CSV_DIR\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ Le dossier {CSV_DIR} est vide. DÃ©marrage du tÃ©lÃ©chargement...\")\n",
    "\n",
    "    # Download latest version\n",
    "    # Note: L'argument 'path' de kagglehub.dataset_download DOIT Ãªtre le chemin\n",
    "    # oÃ¹ le dataset sera tÃ©lÃ©chargÃ© (CSV_DIR dans ce cas)\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(handle=KAGGLE_HANDLE, path=CSV_DIR)\n",
    "        print(\"TÃ©lÃ©chargement rÃ©ussi!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du tÃ©lÃ©chargement Kaggle : {e}\")\n",
    "        path = None\n",
    "\n",
    "\n",
    "# --- 3. Affichage du RÃ©sultat ---\n",
    "if path:\n",
    "    print(\"Chemin d'accÃ¨s aux fichiers du dataset:\", os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import Iterator, Generator, List, Dict\n",
    "\n",
    "# --- Constantes ---\n",
    "CSV_FILE_PATH = None\n",
    "CHUNK_SIZE = 100000  # DÃ©finir une taille de morceau (par exemple, 100 000 lignes)\n",
    "\n",
    "def process_csv_by_chunks(path: str, chunk_size: int) -> List[pd.DataFrame] | None:\n",
    "    \"\"\"\n",
    "    Charge un fichier CSV par morceaux, traite chaque morceau,\n",
    "    et retourne une liste des DataFrames traitÃ©s.\n",
    "\n",
    "    Args:\n",
    "        path (str): Le chemin vers le fichier CSV.\n",
    "        chunk_size (int): Le nombre de lignes Ã  lire Ã  la fois.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame] | None: Une liste des DataFrames traitÃ©s, ou None en cas d'erreur.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Tentative de chargement du fichier : {os.path.abspath(path)}\")\n",
    "    print(f\"Chargement par morceaux de taille : {chunk_size} lignes.\")\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Erreur: Le fichier '{path}' est introuvable. VÃ©rifiez le chemin d'accÃ¨s.\")\n",
    "        return None\n",
    "\n",
    "    processed_chunks = []\n",
    "    chunk_index = 0\n",
    "\n",
    "    try:\n",
    "        # CrÃ©er un itÃ©rateur (TextFileReader) au lieu d'un DataFrame unique\n",
    "        csv_iterator = pd.read_csv(path, chunksize=chunk_size)\n",
    "\n",
    "        # Parcourir les morceaux gÃ©nÃ©rÃ©s par l'itÃ©rateur\n",
    "        for chunk in csv_iterator:\n",
    "            chunk_index += 1\n",
    "            print(f\"Traitement du morceau #{chunk_index} (taille: {len(chunk)} lignes)...\")\n",
    "\n",
    "            # ðŸ’¡ --- Zone de Traitement des DonnÃ©es --- ðŸ’¡\n",
    "            # Ici, vous pouvez appliquer des opÃ©rations qui rÃ©duisent la taille du morceau,\n",
    "            # comme le filtrage, l'agrÃ©gation ou le calcul de statistiques.\n",
    "\n",
    "            # Exemple : Calculer la moyenne de toutes les colonnes et stocker\n",
    "            # stats_df = chunk.mean().to_frame().T\n",
    "            # processed_chunks.append(stats_df)\n",
    "\n",
    "            # Exemple : Filtrer pour garder uniquement les lignes oÃ¹ 'col_A' > 10\n",
    "            # filtered_chunk = chunk[chunk['col_A'] > 10]\n",
    "            # processed_chunks.append(filtered_chunk)\n",
    "\n",
    "            # --- Fin de la Zone de Traitement ---\n",
    "\n",
    "            # Dans cet exemple, nous stockons le morceau complet filtrÃ©\n",
    "            # Si vous avez 500Mo, vous DEVEZ faire un traitement pour rÃ©duire le morceau avant de l'ajouter\n",
    "            # Ã  'processed_chunks', sinon vous resaturerez votre RAM.\n",
    "            processed_chunks.append(chunk)\n",
    "\n",
    "\n",
    "        print(f\"\\nChargement et traitement terminÃ©s. {chunk_index} morceaux traitÃ©s.\")\n",
    "\n",
    "        # âš ï¸ ATTENTION : La ligne suivante va CONSOLIDER TOUS les morceaux.\n",
    "        # Si vous n'avez pas rÃ©duit la taille des morceaux, vous risquez une saturation RAM.\n",
    "        # Si vous n'avez besoin que de statistiques, vous pouvez retourner processed_chunks directement.\n",
    "        final_dataframe = pd.concat(processed_chunks, ignore_index=True)\n",
    "        print(f\"Taille du DataFrame final: {len(final_dataframe)} lignes.\")\n",
    "        return final_dataframe\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors du traitement du fichier CSV : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5cf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "ðŸ“– DÃ©marrage du chargement pour : TEP_Faulty_Training.csv\n",
      "   (Chunk size: 100000, Sampling: 1/10)...\n",
      "=======================================================\n",
      "-> Traitement du bloc nÂ°1 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°2 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°3 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°4 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°5 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°6 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°7 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°8 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°9 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°10 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°11 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°12 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°13 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°14 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°15 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°16 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°17 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°18 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°19 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°20 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°21 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°22 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°23 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°24 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°25 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°26 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°27 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°28 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°29 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°30 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°31 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°32 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°33 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°34 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°35 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°36 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°37 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°38 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°39 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°40 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°41 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°42 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°43 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°44 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°45 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°46 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°47 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°48 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°49 (100000 lignes)...\n",
      "-> Traitement du bloc nÂ°50 (100000 lignes)...\n",
      "âœ… Fin du traitement des blocs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- PARAMÃˆTRES GLOBAUX ---\n",
    "CHUNK_SIZE = 100000  # Taille de chaque bloc de lecture (ex: 100 000 lignes)\n",
    "SAMPLING_STEP = 10    # Garder 1 ligne sur 10\n",
    "OUTPUT_DIR = '../raw_data/sampled_data_1_10/' # RÃ©pertoire de destination pour les fichiers Ã©chantillonnÃ©s\n",
    "\n",
    "# Assurez-vous que le rÃ©pertoire de sortie existe\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def process_csv_by_chunks(path: str, chunk_size: int, sampling_step: int = SAMPLING_STEP) -> None:\n",
    "    \"\"\"\n",
    "    Charge un grand fichier CSV par blocs, Ã©chantillonne chaque bloc,\n",
    "    puis concatÃ¨ne et sauvegarde le DataFrame final Ã©chantillonnÃ©.\n",
    "    \"\"\"\n",
    "    file_path = Path(path)\n",
    "\n",
    "    # --- VÃ‰RIFICATION ---\n",
    "    if not file_path.is_file():\n",
    "        print(f\"ðŸš¨ ERREUR : Le fichier spÃ©cifiÃ© n'existe pas : {path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n=======================================================\")\n",
    "    print(f\"ðŸ“– DÃ©marrage du chargement pour : {file_path.name}\")\n",
    "    print(f\"   (Chunk size: {chunk_size}, Sampling: 1/{sampling_step})...\")\n",
    "    print(f\"=======================================================\")\n",
    "\n",
    "    # Initialise une liste pour stocker les DataFrames Ã©chantillonnÃ©s\n",
    "    sampled_chunks = []\n",
    "    total_rows_processed = 0\n",
    "\n",
    "    # Utilisez un itÃ©rateur pour lire le fichier CSV par blocs\n",
    "    try:\n",
    "        csv_iterator = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸš¨ ERREUR lors de la lecture du CSV : {e}\")\n",
    "        return\n",
    "\n",
    "    # --- BOUCLE DE TRAITEMENT ---\n",
    "    for i, chunk in enumerate(csv_iterator):\n",
    "        print(f\"-> Traitement du bloc nÂ°{i+1} ({len(chunk)} lignes)...\")\n",
    "\n",
    "        # 1. Ã‰chantillonnage : Garder 1 ligne sur SAMPLING_STEP\n",
    "        # df.iloc[::step, :] est la syntaxe correcte pour l'Ã©chantillonnage des lignes.\n",
    "        sampled_chunk = chunk.iloc[::sampling_step, :]\n",
    "\n",
    "        # 2. Ajout du bloc Ã©chantillonnÃ© Ã  la liste\n",
    "        sampled_chunks.append(sampled_chunk)\n",
    "\n",
    "        total_rows_processed += len(chunk)\n",
    "\n",
    "    print(\"âœ… Fin du traitement des blocs.\")\n",
    "\n",
    "    # --- CONCATÃ‰NATION FINALE ---\n",
    "    # ConcatÃ¨ne tous les DataFrames Ã©chantillonnÃ©s en un seul DataFrame final\n",
    "    final_df = pd.concat(sampled_chunks, ignore_index=True)\n",
    "\n",
    "    # --- SAUVEGARDE DU RÃ‰SULTAT ---\n",
    "\n",
    "    # CrÃ©ation du nouveau nom de fichier (ex: TEP_FaultFree_Testing_sampled.csv)\n",
    "    new_file_name = f\"{file_path.stem}_sampled{file_path.suffix}\"\n",
    "    output_path = Path(OUTPUT_DIR) / new_file_name\n",
    "\n",
    "    final_df.to_csv(output_path, index=False) # index=False pour ne pas sauvegarder l'index du DataFrame\n",
    "\n",
    "    # --- RÃ‰SULTAT ---\n",
    "    print(\"\\n--- RÃ‰SULTAT FINAL ---\")\n",
    "    print(f\"Total des lignes traitÃ©es : {total_rows_processed}\")\n",
    "    print(f\"Lignes dans le DataFrame final (Ã©chantillonnÃ©) : {len(final_df)}\")\n",
    "    print(f\"Fichier sauvegardÃ© sous : {output_path}\")\n",
    "    print(\"----------------------\")\n",
    "\n",
    "\n",
    "# --- EXÃ‰CUTION DU CODE ---\n",
    "pathlist = [\n",
    "    # '/home/bapt/code/Monitor-the-Reactor/raw_data/TEP_FaultFree_Testing.csv',\n",
    "    # '/home/bapt/code/Monitor-the-Reactor/raw_data/TEP_FaultFree_Training.csv',\n",
    "    # '/home/bapt/code/Monitor-the-Reactor/raw_data/TEP_Faulty_Testing.csv',\n",
    "    '/home/bapt/code/Monitor-the-Reactor/raw_data/TEP_Faulty_Training.csv'\n",
    "]\n",
    "\n",
    "# Lance le traitemÂµent pour chaque fichier de la liste\n",
    "for path in pathlist:\n",
    "    process_csv_by_chunks(path, chunk_size=CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96955cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monitor-the-reactor-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
