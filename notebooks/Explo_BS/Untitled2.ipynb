{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPPx4FQy22hg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from typing import Iterator, Generator, List, Dict\n",
        "\n",
        "# --- Constantes ---\n",
        "CSV_FILE_PATH = None\n",
        "CHUNK_SIZE = 100000  # D√©finir une taille de morceau (par exemple, 100 000 lignes)\n",
        "\n",
        "def process_csv_by_chunks(path: str, chunk_size: int) -> List[pd.DataFrame] | None:\n",
        "    \"\"\"\n",
        "    Charge un fichier CSV par morceaux, traite chaque morceau,\n",
        "    et retourne une liste des DataFrames trait√©s.\n",
        "\n",
        "    Args:\n",
        "        path (str): Le chemin vers le fichier CSV.\n",
        "        chunk_size (int): Le nombre de lignes √† lire √† la fois.\n",
        "\n",
        "    Returns:\n",
        "        List[pd.DataFrame] | None: Une liste des DataFrames trait√©s, ou None en cas d'erreur.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Tentative de chargement du fichier : {os.path.abspath(path)}\")\n",
        "    print(f\"Chargement par morceaux de taille : {chunk_size} lignes.\")\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Erreur: Le fichier '{path}' est introuvable. V√©rifiez le chemin d'acc√®s.\")\n",
        "        return None\n",
        "\n",
        "    processed_chunks = []\n",
        "    chunk_index = 0\n",
        "\n",
        "    try:\n",
        "        # Cr√©er un it√©rateur (TextFileReader) au lieu d'un DataFrame unique\n",
        "        csv_iterator = pd.read_csv(path, chunksize=chunk_size)\n",
        "\n",
        "        # Parcourir les morceaux g√©n√©r√©s par l'it√©rateur\n",
        "        for chunk in csv_iterator:\n",
        "            chunk_index += 1\n",
        "            print(f\"Traitement du morceau #{chunk_index} (taille: {len(chunk)} lignes)...\")\n",
        "\n",
        "            # üí° --- Zone de Traitement des Donn√©es --- üí°\n",
        "            # Ici, vous pouvez appliquer des op√©rations qui r√©duisent la taille du morceau,\n",
        "            # comme le filtrage, l'agr√©gation ou le calcul de statistiques.\n",
        "\n",
        "            # Exemple : Calculer la moyenne de toutes les colonnes et stocker\n",
        "            # stats_df = chunk.mean().to_frame().T\n",
        "            # processed_chunks.append(stats_df)\n",
        "\n",
        "            # Exemple : Filtrer pour garder uniquement les lignes o√π 'col_A' > 10\n",
        "            # filtered_chunk = chunk[chunk['col_A'] > 10]\n",
        "            # processed_chunks.append(filtered_chunk)\n",
        "\n",
        "            # --- Fin de la Zone de Traitement ---\n",
        "\n",
        "            # Dans cet exemple, nous stockons le morceau complet filtr√©\n",
        "            # Si vous avez 500Mo, vous DEVEZ faire un traitement pour r√©duire le morceau avant de l'ajouter\n",
        "            # √† 'processed_chunks', sinon vous resaturerez votre RAM.\n",
        "            processed_chunks.append(chunk)\n",
        "\n",
        "\n",
        "        print(f\"\\nChargement et traitement termin√©s. {chunk_index} morceaux trait√©s.\")\n",
        "\n",
        "        # ‚ö†Ô∏è ATTENTION : La ligne suivante va CONSOLIDER TOUS les morceaux.\n",
        "        # Si vous n'avez pas r√©duit la taille des morceaux, vous risquez une saturation RAM.\n",
        "        # Si vous n'avez besoin que de statistiques, vous pouvez retourner processed_chunks directement.\n",
        "        final_dataframe = pd.concat(processed_chunks, ignore_index=True)\n",
        "        print(f\"Taille du DataFrame final: {len(final_dataframe)} lignes.\")\n",
        "        return final_dataframe\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Une erreur s'est produite lors du traitement du fichier CSV : {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ifDlGwnA6IB6"
      },
      "outputs": [],
      "source": [
        "# FF_test = process_csv_by_chunks(\"/content/drive/MyDrive/Colab Notebooks/raw_data/TEP_FaultFree_Testing.csv\",\n",
        "#                                 chunk_size=CHUNK_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TcLDgE9n7Sd-"
      },
      "outputs": [],
      "source": [
        "# FF_train = process_csv_by_chunks(\"/content/drive/MyDrive/Colab Notebooks/raw_data/TEP_FaultFree_Training.csv\",\n",
        "#                                 chunk_size=CHUNK_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WR_qOXkR9LC6"
      },
      "outputs": [],
      "source": [
        "# F_test = process_csv_by_chunks(\"/content/drive/MyDrive/Colab Notebooks/raw_data/TEP_Faulty_Testing.csv\",\n",
        "#                                 chunk_size=CHUNK_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Oz8sdjcR9v3G"
      },
      "outputs": [],
      "source": [
        "# F_train = process_csv_by_chunks(\"/content/drive/MyDrive/Colab Notebooks/raw_data/TEP_Faulty_Training.csv\",\n",
        "#                                 chunk_size=CHUNK_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrQlj5kuuhYj"
      },
      "outputs": [],
      "source": [
        "FF_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqsN7pZFt-yP"
      },
      "outputs": [],
      "source": [
        "def reduce_sim(df, nb_sim):\n",
        "  '''\n",
        "  Permet de reduire le nombre de lignes de simulations\n",
        "  '''\n",
        "  print('\\n Shape du df before : ')\n",
        "  print(df.shape)\n",
        "  df = df[df['simulationRun'] <= nb_sim]\n",
        "\n",
        "  print('\\n Shape du df apr√®s : ')\n",
        "  print(df.shape)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7UZ6UVovFlm"
      },
      "outputs": [],
      "source": [
        "FF_train_20sim = reduce_sim(FF_train, 20)\n",
        "F_train_20sim = reduce_sim(F_train, 20)\n",
        "FF_test_20sim = reduce_sim(FF_test, 20)\n",
        "F_test_20sim = reduce_sim(F_test, 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai5VEPB-vQM7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfvujraTZLu6"
      },
      "outputs": [],
      "source": [
        "faulty_train = F_train_20sim\n",
        "faulty_test = F_test_20sim\n",
        "fault_free_train = FF_train_20sim\n",
        "fault_free_test = FF_test_20sim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZeS181IbSld",
        "outputId": "22987685-0a43-458d-a2d2-bd950ed114c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PR√âPARATION DES DONN√âES\n",
            "============================================================\n",
            "Conversion en 3D...\n",
            "Longueur cible: 19200\n",
            "\n",
            "X_train_full: (40, 19200, 52)\n",
            "X_test_final: (40, 19200, 52)\n",
            "Distribution train: [20 20]\n",
            "Distribution test: [20 20]\n",
            "\n",
            "Train: (32, 19200, 52), Val: (8, 19200, 52), Test: (40, 19200, 52)\n",
            "\n",
            "============================================================\n",
            "MOD√àLE LSTM\n",
            "============================================================\n",
            "Epoch 1/50\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Vos donn√©es sont d√©j√† charg√©es\n",
        "# fault_free_train, faulty_train, fault_free_test, faulty_test\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PR√âPARATION DES DONN√âES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 1. CONVERSION EN 3D\n",
        "# ====================================\n",
        "\n",
        "def dataframe_to_3d(df, has_fault=True):\n",
        "    \"\"\"Convertit DataFrame en 3D\"\"\"\n",
        "    cols_to_drop = ['faultNumber', 'simulationRun', 'sample']\n",
        "    feature_cols = [col for col in df.columns if col not in cols_to_drop]\n",
        "\n",
        "    df_sorted = df.sort_values(['simulationRun', 'sample']).reset_index(drop=True)\n",
        "    simulation_ids = df_sorted['simulationRun'].unique()\n",
        "\n",
        "    simulations = []\n",
        "    labels = []\n",
        "\n",
        "    for sim_id in simulation_ids:\n",
        "        sim_data = df_sorted[df_sorted['simulationRun'] == sim_id]\n",
        "        features = sim_data[feature_cols].values\n",
        "        simulations.append(features)\n",
        "        labels.append(sim_data['faultNumber'].iloc[0] if has_fault else 0)\n",
        "\n",
        "    return simulations, np.array(labels, dtype=int)\n",
        "\n",
        "def pad_or_truncate(simulations, target_length):\n",
        "    \"\"\"Harmonise les longueurs\"\"\"\n",
        "    result = []\n",
        "    for sim in simulations:\n",
        "        if len(sim) < target_length:\n",
        "            padding = np.repeat([sim[-1]], target_length - len(sim), axis=0)\n",
        "            result.append(np.vstack([sim, padding]))\n",
        "        elif len(sim) > target_length:\n",
        "            result.append(sim[:target_length])\n",
        "        else:\n",
        "            result.append(sim)\n",
        "    return np.array(result)\n",
        "\n",
        "# Convertir en 3D\n",
        "print(\"Conversion en 3D...\")\n",
        "X_ff_tr_list, y_ff_tr = dataframe_to_3d(fault_free_train, False)\n",
        "X_f_tr_list, y_f_tr = dataframe_to_3d(faulty_train, True)\n",
        "X_ff_te_list, y_ff_te = dataframe_to_3d(fault_free_test, False)\n",
        "X_f_te_list, y_f_te = dataframe_to_3d(faulty_test, True)\n",
        "\n",
        "# Harmoniser les longueurs\n",
        "target_length = max(max(len(s) for s in X_f_tr_list), max(len(s) for s in X_f_te_list))\n",
        "print(f\"Longueur cible: {target_length}\")\n",
        "\n",
        "X_ff_tr = pad_or_truncate(X_ff_tr_list, target_length)\n",
        "X_f_tr = pad_or_truncate(X_f_tr_list, target_length)\n",
        "X_ff_te = pad_or_truncate(X_ff_te_list, target_length)\n",
        "X_f_te = pad_or_truncate(X_f_te_list, target_length)\n",
        "\n",
        "# Combiner\n",
        "X_train_full = np.concatenate([X_ff_tr, X_f_tr], axis=0)\n",
        "y_train_full = np.concatenate([y_ff_tr, y_f_tr])\n",
        "X_test_final = np.concatenate([X_ff_te, X_f_te], axis=0)\n",
        "y_test_final = np.concatenate([y_ff_te, y_f_te])\n",
        "\n",
        "# M√©langer\n",
        "X_train_full, y_train_full = shuffle(X_train_full, y_train_full, random_state=42)\n",
        "X_test_final, y_test_final = shuffle(X_test_final, y_test_final, random_state=42)\n",
        "\n",
        "print(f\"\\nX_train_full: {X_train_full.shape}\")\n",
        "print(f\"X_test_final: {X_test_final.shape}\")\n",
        "print(f\"Distribution train: {np.bincount(y_train_full)}\")\n",
        "print(f\"Distribution test: {np.bincount(y_test_final)}\")\n",
        "\n",
        "# ====================================\n",
        "# 2. SPLIT ET NORMALISATION\n",
        "# ====================================\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=42\n",
        ")\n",
        "\n",
        "n_train, n_time, n_feat = X_train.shape\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train.reshape(-1, n_feat)).reshape(n_train, n_time, n_feat)\n",
        "X_val = scaler.transform(X_val.reshape(-1, n_feat)).reshape(X_val.shape[0], n_time, n_feat)\n",
        "X_test_final = scaler.transform(X_test_final.reshape(-1, n_feat)).reshape(X_test_final.shape[0], X_test_final.shape[1], n_feat)\n",
        "\n",
        "print(f\"\\nTrain: {X_train.shape}, Val: {X_val.shape}, Test: {X_test_final.shape}\")\n",
        "\n",
        "# ====================================\n",
        "# 3. MOD√àLE\n",
        "# ====================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MOD√àLE LSTM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(n_time, n_feat)),\n",
        "    layers.LSTM(128, return_sequences=True),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.LSTM(64),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(21, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ====================================\n",
        "# 4. ENTRA√éNEMENT\n",
        "# ====================================\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ====================================\n",
        "# 5. √âVALUATION - CORRIG√âE\n",
        "# ====================================\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"TEST ACCURACY: {test_acc:.4f}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "y_pred = model.predict(X_test_final, verbose=0)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Classes pr√©sentes\n",
        "unique_labels = sorted(np.unique(y_test_final))\n",
        "class_names = ['Normal' if l==0 else f'Panne_{l}' for l in unique_labels]\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\n\" + classification_report(\n",
        "    y_test_final, y_pred_classes,\n",
        "    labels=unique_labels,\n",
        "    target_names=class_names,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# Sauvegarder le mod√®le\n",
        "model.save('lstm_baseline_model.keras')\n",
        "print(\"\\n‚úì Mod√®le sauvegard√© : lstm_baseline_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG0f_YVvwD0Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
